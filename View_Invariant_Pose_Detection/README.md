Viewpoint Invariant Human Pose Estimation

We propose to work on a model to improve the accuracy to achieve viewpoint invariant model
for human pose estimation from a single depth image.This approach tries to embed local patches
into a learned viewpoint invariant feature space by computing 12 transformation parameters
using spatial transformation network.The model learns to predict the poses in the presence of
noise and occlusion by selectively extracting glimpses from each body-joints and predicting the
presence of the same.This method uses convolutional(VGG-16) and recurrent(LSTM) network
to predict joint locations. To achieve more accuracy we have used iterative error feedback
technique which iteratively tries to move joint locations by learning from previous iteration.
We finally train our model on Invariant-Top View Dataset (ITOP) which consists of 100K
real-world depth images taken from various camera viewpoint.

Various methods have been proposed in human pose estimation like deformable part models
[FGMR10] , pictorial structures[FH05] to model each body joint independently.Convolutional
network which have provided state-of-art result in various vision problem failed to work with
human pose estimation as pose represent lower dimensional manifold in the high dimensional input
space.So [FZLW15, LLC14] first detect the body parts and then localizes the parts.Another
work is based on enforcing global pose consistency on Markov random fields which represents
human anatomical constraints[TJLB14].Graphical models are also popular in human pose estimation
which encodes dependencies between outputs[CY14].
Since availability of depth maps for input image use of iterative closest point algorithms[GPKT12,
GWK05] and database lookups [YWY+11] is widely motivated. [HWLX15] imposes kinematic
constraints for improving human pose estimation.With this kernel methods with kinematic
chain structures and template fitting models have been proposed.Various discriminative approaches
have shown good results.Body segmentation from a single depth image using random
forest classifier is used to predict body part location[SSK+13].Other works extending this work
includes hough forests[G
Chapter 3
Model
3.1 Model Architecture
3.1.1 Input
The input to our model is depth image of dimension 240 x 320.We extract a set of 15 patches
from the image with each centered around the predicted location of the joints.We denote the
pose by locating 15 joints.Each of the patches is foveated from the center to form a glimpse.Each
glimpse is generated by the predicted location from previous iteration ytâˆ’1.We initialize the
location to the average pose y0.